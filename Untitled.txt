import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.interpolate import make_interp_spline

# Create a DataFrame with your data
data = {
    'x': [0.5, 1.5, 2.5,3.5,4.5],  # Midpoints of the histogram bins
    'y': [10, 8, 6,5,4]        # Corresponding frequencies or densities
}
df = pd.DataFrame(data)

# Extract x and y values from the DataFrame
x_values = df['x']
y_values = df['y']

# Interpolate the midpoints to create a smooth curve
x_smooth = np.linspace(x_values.min(), x_values.max(), 300)
spline = make_interp_spline(x_values, y_values, k=3)  # Cubic spline interpolation
y_smooth = spline(x_smooth)

# Plot the histogram (using bar plot since data is already grouped)
plt.bar(x_values, y_values, width=1.0, color='lightblue', edgecolor='blue', alpha=0.6, label='Histogram')

# Plot the smooth curve
plt.plot(x_smooth, y_smooth, color='red', label='Interpolated Curve')

# Add labels and legend
plt.xlabel('X Values')
plt.ylabel('Y Values')
plt.title('Histogram with Interpolated Curve')
plt.legend()

# Show the plot
plt.show()



import numpy as np
from scipy.optimize import minimize

# Example data
X = np.array([
    [1, 2],
    [2, 3],
    [3, 4],
    [4, 5],
    [5, 6]
])  # Feature matrix (5 samples, 2 features)

Y = np.array([
    [30, 20],
    [25, 30],
    [40, 40],
    [45, 50],
    [10, 60]
])  # Target matrix (5 samples, 2 outputs)

# Dimensions
n_samples, n_features = X.shape
_, n_outputs = Y.shape


# Objective function: Minimize sum of squared errors (Frobenius norm)
def objective(beta_flat):
    beta = beta_flat.reshape(n_features, n_outputs)  # Reshape beta back to 2D
    predictions = X @ beta  # Predicted matrix
    return np.sum((predictions - Y) ** 2)  # Frobenius norm

# Constraints: Bounds on predictions (X @ beta)
prediction_constraints = [
    {'type': 'ineq', 'fun': lambda beta_flat, i=i, j=j: (
        (X @ beta_flat.reshape(n_features, n_outputs))[i, j+1] -
        (X @ beta_flat.reshape(n_features, n_outputs))[i, j]
    )}  # Monotonicity constraint for feature dimension
    for i in range(n_samples) for j in range(n_outputs - 1)
] + [
    {'type': 'ineq', 'fun': lambda beta_flat, i=i, j=j: (
        (beta_flat.reshape(n_features, n_outputs))[i, j+1] -
        (beta_flat.reshape(n_features, n_outputs))[i, j]
    )}
    for i in range(n_features) for j in range(n_outputs - 1)
]


# Initial guess for beta
initial_guess = np.zeros(n_features * n_outputs)  # Flattened initial guess

# Solve the optimization problem
result = minimize(
    objective,                     # Objective function
    initial_guess,                 # Initial guess
    method='SLSQP',                # Sequential Least Squares Programming
    constraints=prediction_constraints  # Constraints on predictions
)

# Extract the optimized coefficients
if result.success:
    beta_optimized = result.x.reshape(n_features, n_outputs)  # Reshape back to 2D
    predictions_optimized = X @ beta_optimized
    print("Optimized Coefficients (beta):")
    print(beta_optimized)
    print("Optimized Predictions (X @ beta):")
    print(predictions_optimized)
else:
    print("Optimization failed:", result.message)
